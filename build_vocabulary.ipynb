{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-23T16:04:20.972072106Z",
     "start_time": "2023-12-23T16:04:20.951951449Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_labels_path = 'data/deepfashion-multimodal/train_captions.json'\n",
    "with open(train_labels_path, 'rb') as fp:\n",
    "    train_labels = json.load(fp)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T15:59:07.397253052Z",
     "start_time": "2023-12-23T15:59:07.385112203Z"
    }
   },
   "id": "3657c8727a5ca400"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building vocabulary: 100%|██████████| 10155/10155 [00:58<00:00, 174.68it/s]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "word_counts = defaultdict(int)\n",
    "\n",
    "word2vec = {}\n",
    "doc_lengths = []\n",
    "for _, text_label in tqdm(train_labels.items(), desc='Building vocabulary'):\n",
    "    doc = nlp(text_label)\n",
    "    doc_lengths.append(len(doc))\n",
    "    for token in doc:\n",
    "        word = token.text.lower()\n",
    "        word_counts[word] += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T16:00:06.547476193Z",
     "start_time": "2023-12-23T15:59:08.137093216Z"
    }
   },
   "id": "8c9e88bf8acdb7f0"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "109"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(doc_lengths)  # 最大的文档长度是109"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T16:00:15.452673492Z",
     "start_time": "2023-12-23T16:00:15.443465194Z"
    }
   },
   "id": "b71390633635e73b"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "vocabulary = {k: i+1 for i, k in enumerate(word_counts.keys())}\n",
    "vocabulary['<pad>'] = 0\n",
    "vocabulary['<unk>'] = len(vocabulary)\n",
    "vocabulary['<start>'] = len(vocabulary)\n",
    "vocabulary['<end>'] = len(vocabulary)\n",
    "vocabulary_inv = {v: k for k, v in vocabulary.items()}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T16:01:07.258281319Z",
     "start_time": "2023-12-23T16:01:07.246434522Z"
    }
   },
   "id": "2350b44c426bc5ec"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "embed_dim = 96"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T16:03:55.197881209Z",
     "start_time": "2023-12-23T16:03:55.185567905Z"
    }
   },
   "id": "42df304309442c1b"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "vecs = []\n",
    "\n",
    "for i in range(len(vocabulary)):\n",
    "    word = vocabulary_inv[i]\n",
    "    if word in word2vec:\n",
    "        vecs.append(word2vec[word])\n",
    "    else:\n",
    "        vecs.append(np.random.rand(96))\n",
    "\n",
    "vecs_arr = np.array(vecs)\n",
    "np.save('word2vec.npy', vecs_arr)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T16:05:12.339153018Z",
     "start_time": "2023-12-23T16:05:12.296989922Z"
    }
   },
   "id": "2b1a2e0d4f6df10f"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(110, 96)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs_arr.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T16:04:35.905867895Z",
     "start_time": "2023-12-23T16:04:35.862943568Z"
    }
   },
   "id": "1f8c86a351c02f4"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# 存储词典\n",
    "with open(os.path.join('data/deepfashion-multimodal', 'vocab.json'), 'w') as fw:\n",
    "    json.dump(vocabulary, fw)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T18:06:11.116052681Z",
     "start_time": "2023-12-22T18:06:11.046798281Z"
    }
   },
   "id": "626bb627d943d8cb"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "{'the': 1,\n 'upper': 2,\n 'clothing': 3,\n 'has': 4,\n 'long': 5,\n 'sleeves': 6,\n ',': 7,\n 'cotton': 8,\n 'fabric': 9,\n 'and': 10,\n 'solid': 11,\n 'color': 12,\n 'patterns': 13,\n '.': 14,\n 'neckline': 15,\n 'of': 16,\n 'it': 17,\n 'is': 18,\n 'v': 19,\n '-': 20,\n 'shape': 21,\n 'lower': 22,\n 'length': 23,\n 'denim': 24,\n 'this': 25,\n 'lady': 26,\n 'also': 27,\n 'wears': 28,\n 'an': 29,\n 'outer': 30,\n 'with': 31,\n 'complicated': 32,\n 'female': 33,\n 'wearing': 34,\n 'a': 35,\n 'ring': 36,\n 'on': 37,\n 'her': 38,\n 'finger': 39,\n 'neckwear': 40,\n 'tank': 41,\n 'shirt': 42,\n 'no': 43,\n 'chiffon': 44,\n 'graphic': 45,\n 'round': 46,\n 'person': 47,\n 'pants': 48,\n 'are': 49,\n 'top': 50,\n 'woman': 51,\n 'trousers': 52,\n 'there': 53,\n 'belt': 54,\n 'accessory': 55,\n 'wrist': 56,\n 'sweater': 57,\n 'lattice': 58,\n 'three': 59,\n 'point': 60,\n 'pure': 61,\n 'in': 62,\n 'his': 63,\n 'neck': 64,\n 'sleeve': 65,\n 'plaid': 66,\n 'its': 67,\n 'lapel': 68,\n 'socks': 69,\n 'shoes': 70,\n 'suspenders': 71,\n 'short': 72,\n 't': 73,\n 'shorts': 74,\n 'crew': 75,\n 'sleeveless': 76,\n 'floral': 77,\n 'hat': 78,\n 'pair': 79,\n 'quarter': 80,\n 'head': 81,\n 'waist': 82,\n 'leather': 83,\n 'pattern': 84,\n 'cut': 85,\n 'off': 86,\n 'medium': 87,\n 'knitting': 88,\n 'gentleman': 89,\n 'other': 90,\n 'mixed': 91,\n 'stripe': 92,\n 'skirt': 93,\n 'striped': 94,\n 'sunglasses': 95,\n 'guy': 96,\n 'stand': 97,\n 'man': 98,\n 'square': 99,\n 'leggings': 100,\n 'furry': 101,\n 'block': 102,\n 'glasses': 103,\n 'hands': 104,\n 'or': 105,\n 'clothes': 106,\n '<pad>': 0,\n '<unk>': 107,\n '<start>': 108,\n '<end>': 109}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T16:07:17.804544767Z",
     "start_time": "2023-12-23T16:07:17.760035724Z"
    }
   },
   "id": "6e401fa75f3974b7"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'Ex', 'a', 'm', 'p', 'le', '▁t', 'e', 'x', 't', '▁', 'to', '▁', 'to', 'k', 'e', 'n', 'i', 'z', 'e']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=all_texts.txt --model_prefix=m --vocab_size=229\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: all_texts.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 229\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: all_texts.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 10155 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=2861462\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9776% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=29\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999776\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 10155 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2308724\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 385 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 10155\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 157\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 157 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=293 obj=10.088 num_tokens=342 num_tokens/piece=1.16724\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=225 obj=8.25742 num_tokens=346 num_tokens/piece=1.53778\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# 将所有文本保存到一个文件中\n",
    "with open('all_texts.txt', 'w') as f:\n",
    "    for _, text_label in train_labels.items():\n",
    "        f.write(text_label + '\\n')\n",
    "\n",
    "# 训练 SentencePiece 模型\n",
    "spm.SentencePieceTrainer.train('--input=all_texts.txt --model_prefix=m --vocab_size=229')\n",
    "\n",
    "# 加载模型\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "\n",
    "# 使用模型\n",
    "tokens = sp.encode_as_pieces('Example text to tokenize')\n",
    "print(tokens)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T16:22:00.046398075Z",
     "start_time": "2023-12-23T16:21:59.452835085Z"
    }
   },
   "id": "79a0054ff8cdb900"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58832/58832 [02:28<00:00, 397.46it/s, max_len=23]\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "with open('data/deepfashion-multimodal/train_captions_split.json', 'rb') as fr:\n",
    "    train_labels_split = json.load(fr)\n",
    "    p_bar = tqdm(train_labels_split)\n",
    "    for _, text in p_bar:\n",
    "        doc = nlp(text)\n",
    "        if len(doc) > max_len:\n",
    "            max_len = len(doc)\n",
    "            p_bar.set_postfix(max_len=max_len)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T18:01:59.648790480Z",
     "start_time": "2023-12-23T17:59:31.608213897Z"
    }
   },
   "id": "e78c1d4c66e1a27e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "23899211c059cc82"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
