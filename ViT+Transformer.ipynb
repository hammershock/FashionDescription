{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 一、导入模块"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9db64e20ca8b29f8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "由于数据集、词表、评估标准当中在两个实验中有很多共用的地方，故编写在`datasets.py`,`vocabualary.py`和`metrics.py`中，不在notebook中体现。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b9ae33b1e2476ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858c5eedbd5b4ea2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from datasets import ImageTextDataset\n",
    "from vocabulary import Vocabulary\n",
    "from metrics import bleu, rouge_l"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 二、定义模型"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8511838a29d1b862"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"ViT嵌入层，通过将原始图像分为若干个小块，分别嵌入，然后展平为序列\"\"\"\n",
    "    def __init__(self, in_channels: int, patch_size: int, emb_size: int, img_size: int):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)  # Shape: (batch_size, emb_size, n_patches^(1/2), n_patches^(1/2))\n",
    "        x = x.flatten(2)  # Shape: (batch_size, emb_size, n_patches)\n",
    "        x = x.transpose(1, 2)  # Shape: (batch_size, n_patches, emb_size)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"编码器、解码器点对点前馈层\"\"\"\n",
    "    def __init__(self, emb_size: int, expansion: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(emb_size, expansion * emb_size)\n",
    "        self.fc2 = nn.Linear(expansion * emb_size, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"ViT编码器层\"\"\"\n",
    "    def __init__(self, emb_size: int, num_heads: int, expansion: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(emb_size)\n",
    "        self.norm2 = nn.LayerNorm(emb_size)\n",
    "        self.attention = nn.MultiheadAttention(emb_size, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(emb_size, expansion, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attention, _ = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(attention + x)\n",
    "        forward = self.feed_forward(x)\n",
    "        x = self.norm2(forward + x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformerEncoder(nn.Module):\n",
    "    \"\"\"ViT编码器\"\"\"\n",
    "    def __init__(self, in_channels: int, patch_size: int, img_size: int, emb_size: int, num_layers: int, num_heads: int,\n",
    "                 expansion: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding(in_channels, patch_size, emb_size, img_size)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, 1 + self.patch_embedding.n_patches, emb_size))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(emb_size, num_heads, expansion, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.patch_embedding(x)\n",
    "        batch_size, _, _ = x.shape\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x += self.positional_embedding\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def create_masks(target_seq, pad_idx, num_heads):\n",
    "    \"\"\"创建目标序列注意力掩码\"\"\"\n",
    "    # 创建三角掩码\n",
    "    seq_len = target_seq.size(1)\n",
    "    triangular_mask = torch.triu(torch.ones((seq_len, seq_len), device=target_seq.device) * float('-inf'), diagonal=1)\n",
    "\n",
    "    # 创建PAD掩码\n",
    "    pad_mask = (target_seq == pad_idx).to(target_seq.device)  # [batch_size, seq_len]\n",
    "    pad_mask = pad_mask.unsqueeze(1).expand(-1, seq_len, -1)  # [batch_size, seq_len, seq_len]\n",
    "\n",
    "    # 合并掩码\n",
    "    tgt_mask = triangular_mask.unsqueeze(0).expand(pad_mask.size(0), -1, -1)  # [batch_size, seq_len, seq_len]\n",
    "    tgt_mask = tgt_mask.masked_fill(pad_mask, float('-inf'))\n",
    "\n",
    "    # 调整掩码形状以适应多头注意力\n",
    "    tgt_mask = tgt_mask.repeat_interleave(num_heads, dim=0)  # [batch_size * num_heads, seq_len, seq_len]\n",
    "\n",
    "    return tgt_mask\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"transformer解码器层\"\"\"\n",
    "    def __init__(self, emb_size, num_heads, expansion, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(emb_size)\n",
    "        self.norm2 = nn.LayerNorm(emb_size)\n",
    "        self.norm3 = nn.LayerNorm(emb_size)\n",
    "        self.self_attention = nn.MultiheadAttention(emb_size, num_heads, dropout=dropout)\n",
    "        self.encoder_attention = nn.MultiheadAttention(emb_size, num_heads, dropout=dropout)\n",
    "        self.feed_forward = FeedForward(emb_size, expansion, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.encoder_decoder_att = None  # (batch, seq_len, image_embed_size)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        # Self Attention\n",
    "        x = x.transpose(0, 1)  # Change shape to [seq_length, batch_size, emb_size]\n",
    "        enc_out = enc_out.transpose(0, 1)\n",
    "\n",
    "        attention_output, _ = self.self_attention(x, x, x, attn_mask=trg_mask)\n",
    "        query = self.dropout(self.norm1(attention_output + x))\n",
    "\n",
    "        # Encoder-Decoder Attention\n",
    "        attention_output, self.encoder_decoder_att = self.encoder_attention(query, enc_out, enc_out, attn_mask=src_mask)\n",
    "        # print(self.encoder_decoder_att.shape)  # (batch, seq_len, image_embed_size)\n",
    "        query = self.dropout(self.norm2(attention_output + query))\n",
    "\n",
    "        # Change shape back to [batch_size, seq_length, emb_size]\n",
    "        query = query.transpose(0, 1)\n",
    "\n",
    "        # Feed Forward\n",
    "        out = self.feed_forward(query)\n",
    "        out = self.dropout(self.norm3(out + query))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"目标序列正余弦位置编码\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Inject position encoding\"\"\"\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"Transformer解码器层\"\"\"\n",
    "    def __init__(self, emb_size, num_heads, expansion, dropout, num_layers, target_vocab_size,\n",
    "                 pretrained_embeddings=None):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.word_embedding = nn.Embedding(target_vocab_size, emb_size)\n",
    "\n",
    "        if pretrained_embeddings is not None:\n",
    "            assert pretrained_embeddings.shape == (target_vocab_size, emb_size), \"预训练嵌入向量尺寸不匹配\"\n",
    "            self.word_embedding = nn.Embedding.from_pretrained(torch.FloatTensor(pretrained_embeddings), freeze=False)\n",
    "        else:\n",
    "            self.word_embedding = nn.Embedding(target_vocab_size, emb_size)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(emb_size)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(emb_size, num_heads, expansion, dropout) for _ in range(num_layers)])\n",
    "        self.fc_out = nn.Linear(emb_size, target_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        x = self.dropout(self.word_embedding(x))\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    \"\"\"img2seq模型\"\"\"\n",
    "    def __init__(self, img_size, in_channels, patch_size, emb_size, target_vocab_size, num_layers, num_heads, expansion, dropout, pretrained_embeddings=None):\n",
    "        super(ImageCaptioningModel, self).__init__()\n",
    "        self.encoder = VisionTransformerEncoder(in_channels=in_channels,\n",
    "                                                patch_size=patch_size,\n",
    "                                                img_size=img_size,\n",
    "                                                emb_size=emb_size,\n",
    "                                                num_layers=num_layers,\n",
    "                                                num_heads=num_heads,\n",
    "                                                expansion=expansion,\n",
    "                                                dropout=dropout)\n",
    "        self.decoder = TransformerDecoder(emb_size=emb_size,  # 简单起见，编码器图像块与解码器文本嵌入使用相同的嵌入维度\n",
    "                                          num_heads=num_heads,\n",
    "                                          expansion=expansion,\n",
    "                                          dropout=dropout,\n",
    "                                          num_layers=num_layers,\n",
    "                                          target_vocab_size=target_vocab_size,\n",
    "                                          pretrained_embeddings=pretrained_embeddings)\n",
    "\n",
    "    def forward(self, images, captions, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param images: [batch_size, in_channels, img_size, img_size]\n",
    "        :param captions: [seq_length, batch_size]\n",
    "        \"\"\"\n",
    "        encoder_output = self.encoder(images)  # [batch_size, n_patches + 1, emb_size]\n",
    "        decoder_output = self.decoder(captions, encoder_output, src_mask, tgt_mask)\n",
    "        return decoder_output  # [seq_length, batch_size, target_vocab_size]\n",
    "\n",
    "    def visualize(self):\n",
    "        att_weights = self.decoder.layers[-1].encoder_decoder_att\n",
    "        if att_weights is not None:\n",
    "            return att_weights[:, -1, 1:]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c08712055e4ba4a9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 三、定义训练循环、预测函数、验证函数和注意力可视化函数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a4de8f0c3763545"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, mask_func, save_path, device, epochs=10,\n",
    "          save_interval=120, pretrained_weights=None, experiment_name='experiment'):\n",
    "    writer = SummaryWriter(f'runs/{experiment_name}')\n",
    "\n",
    "    os.makedirs(os.path.split(save_path)[0], exist_ok=True)\n",
    "\n",
    "    if pretrained_weights:\n",
    "        model.load_state_dict(torch.load(pretrained_weights))\n",
    "\n",
    "    # training loop\n",
    "    p_bar = tqdm(range(epochs))\n",
    "    model = model.to(device)\n",
    "    save_interval = timedelta(seconds=save_interval)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in p_bar:\n",
    "        running_loss = 0.0\n",
    "\n",
    "        last_save_time = datetime.now()\n",
    "        for batch_idx, (image, seq, seq_len) in enumerate(train_loader):\n",
    "            image = image.to(device)  # (batch, c, img_sz, img_sz)\n",
    "            seq = seq.to(device)  # (batch, seq_len + 1)\n",
    "\n",
    "            input_seq = seq[:, :-1]  # (batch, seq_len)\n",
    "            target_seq = seq[:, 1:]  # (batch, seq_len)\n",
    "\n",
    "            # 开始训练\n",
    "            optimizer.zero_grad()\n",
    "            tgt_mask = mask_func(input_seq)\n",
    "\n",
    "            prediction = model(image, input_seq, tgt_mask=tgt_mask)  # (batch, seq_len, vocabulary_size)\n",
    "            batch_size, _, vocab_size = prediction.shape\n",
    "            loss = criterion(prediction.view(-1, vocab_size), target_seq.contiguous().view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # autosave\n",
    "            if datetime.now() - last_save_time > save_interval:\n",
    "                last_save_time = datetime.now()\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "\n",
    "            # 记录结果\n",
    "            running_loss += loss.item()\n",
    "            p_bar.set_postfix(progress=f'{(batch_idx + 1)} / {len(train_loader)}',\n",
    "                              loss=f'{running_loss / (batch_idx + 1):.4f}',\n",
    "                              last_save_time=last_save_time)\n",
    "            writer.add_scalar('Loss/train', running_loss / (batch_idx + 1), epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def predict(model, original_image, transform, vocab, device, max_length, mask_func, visualize=False):\n",
    "    model.to(device)\n",
    "    image = transform(original_image).unsqueeze(0).to(device)  # -> (1, channel, img_size, img_size)\n",
    "    seq = [vocab.start]\n",
    "\n",
    "    seq = torch.tensor(seq, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            tgt_mask = mask_func(seq)\n",
    "            output = model(image, seq, tgt_mask=tgt_mask)\n",
    "        if visualize:\n",
    "            att_img = model.visualize()  # torch.Size([1, 196])\n",
    "            visualize_attention(original_image, att_img.squeeze(0), current_word=vocab.inv[seq[:, -1].item()])\n",
    "\n",
    "        predicted = output.argmax(2)[:, -1]\n",
    "        seq = torch.cat((seq, predicted.unsqueeze(1)), dim=1)\n",
    "\n",
    "        if predicted.item() == vocab.end:\n",
    "            break\n",
    "\n",
    "    # 将序列转换为单词列表'\n",
    "    seq = seq.squeeze(0).cpu().numpy().tolist()\n",
    "    text = vocab.decode(seq)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def visualize_attention(origin_image, att_img, current_word: str, img_size=224, patch_size=16):\n",
    "    \"\"\"\n",
    "    可视化注意力权重为热力图，并与原始图像进行对比，同时显示当前生成的词。\n",
    "\n",
    "    :param origin_image: 原始图像\n",
    "    :param att_img: 注意力权重，形状为 [1, num_patches]\n",
    "    :param current_word: 当前生成的词\n",
    "    :param img_size: 输入图像的大小\n",
    "    :param patch_size: 每个块的大小\n",
    "    \"\"\"\n",
    "    # 计算每个维度的块数\n",
    "    num_patches_side = img_size // patch_size\n",
    "\n",
    "    # 重新调整注意力权重的形状以匹配图像的尺寸\n",
    "    attention_map = att_img.reshape(num_patches_side, num_patches_side).cpu().detach().numpy()\n",
    "\n",
    "    # 设置绘图布局\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # 绘制原始图像\n",
    "    axes[0].imshow(origin_image)  # 假设 origin_image 已经是 (H, W, C) 格式\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # 绘制热力图\n",
    "    im = axes[1].imshow(origin_image)  # 首先绘制原始图像\n",
    "    axes[1].imshow(attention_map, cmap='viridis', alpha=0.6)  # 再绘制热力图\n",
    "    axes[1].set_title(f\"Attention for '{current_word}'\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def validate(model, val_set, transform, vocab, mask_func, device, max_length):\n",
    "    pass\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "530af6d1c70e6a21"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 四、模型和训练超参数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df425e9f357c0b30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =================== Example parameters for the model ===================\n",
    "img_size = 224  # Size of the input image\n",
    "in_channels = 3  # Number of input channels (for RGB images)\n",
    "patch_size = 16  # Size of each patch\n",
    "emb_size = 96  # Embedding size\n",
    "num_layers = 6  # Number of layers in both encoder and decoder\n",
    "num_heads = 8  # Number of attention heads\n",
    "expansion = 4  # Expansion factor for feed forward network\n",
    "\n",
    "# =================== Train Config ===================\n",
    "dropout = 0.1  # Dropout rate\n",
    "lr = 5e-4  # Learning rate\n",
    "epochs = 500\n",
    "batch_size = 64  # Batch size\n",
    "seq_length = 128  # Max length of the caption sequence\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "save_path = 'models/model_transformer.pth'\n",
    "experiment_name = 'fashion_description'\n",
    "vocabulary_path = 'vocabulary/vocab.json'\n",
    "word2vec_cache_path = 'vocabulary/word2vec.npy'\n",
    "dataset_root = 'data/deepfashion-multimodal'\n",
    "train_labels_path = 'data/deepfashion-multimodal/train_captions.json'\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "332bcdd1a0e546d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 五、定义词表和图像预处理变换"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9a6974133d43cf1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =================== Vocabulary and Image transforms ===================\n",
    "vocabulary = Vocabulary(vocabulary_path)\n",
    "transform = Compose([\n",
    "    Resize((img_size, img_size)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "mask_func = partial(create_masks, pad_idx=vocabulary.pad, num_heads=num_heads)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5f70bf16111a5e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 六、加载/初始化模型，数据集"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84cff0988339cbeb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =================== Initialize the model ===================\n",
    "model = ImageCaptioningModel(img_size, in_channels, patch_size, emb_size, len(vocabulary), num_layers, num_heads,\n",
    "                             expansion, dropout,\n",
    "                             pretrained_embeddings=vocabulary.get_word2vec(cache_path=word2vec_cache_path))\n",
    "\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "# =================== Prepare for Training ===================\n",
    "\n",
    "dataset = ImageTextDataset(dataset_root,\n",
    "                           train_labels_path,\n",
    "                           vocabulary=vocabulary,\n",
    "                           max_seq_len=seq_length,\n",
    "                           transform=transform,\n",
    "                           max_cache_memory=32 * 1024 ** 3)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cf35255d94041fb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 七、开始训练"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2435ffbd1911b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# =================== Start Training ===================\n",
    "train(model, dataloader, criterion, optimizer, mask_func, save_path='models/model_transformer.pth',\n",
    "      epochs=epochs, device=device, experiment_name=experiment_name)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f118dc2903320be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 八、模型推理和评估"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ffd476c49b8afdf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#  =================== Model Inference ===================\n",
    "image_path, caption = dataset.sample()\n",
    "\n",
    "caption_generated = predict(model, Image.open(image_path), transform, vocabulary, device, seq_length, mask_func,\n",
    "                            visualize=True)\n",
    "print(caption, '\\n\\n', caption_generated)\n",
    "print('bleu score: ', bleu(caption_generated, caption, vocabulary))\n",
    "print('rouge-l score: ', rouge_l(caption_generated, caption, vocabulary))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
