{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 一、导入依赖"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d5c373059adc2fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "由于数据集、词表、评估标准当中在两个实验中有很多共用的地方，故编写在`datasets.py`,`vocabualary.py`和`metrics.py`中，不在notebook中体现。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb95280279ab9c15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b88b2d27455f6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, Normalize, ToTensor\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "from datasets import ImageTextDataset\n",
    "from vocabulary import Vocabulary\n",
    "from metrics import bleu, rouge_l"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 二、定义模型结构"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec149dcd8f864547"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_dim):\n",
    "        \"\"\"\n",
    "        CNN图像编码器，得到图像整体特征\n",
    "        :param out_dim: 输出特征维度\n",
    "        :param in_channels: 图像通道数\n",
    "        \"\"\"\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "        out_shape = (256, 8, 8)  # 根据实际输出尺寸进行调整\n",
    "        self.fc = nn.Linear(in_features=out_shape[0] * out_shape[1] * out_shape[2], out_features=out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)  # (batch, 128, 8, 8)\n",
    "        x = x.view(x.size(0), -1)  # Flatten, (batch, 128*8*8)\n",
    "        x = self.fc(x)\n",
    "        return x  # (batch, out_dim)\n",
    "\n",
    "\n",
    "class ImageTextModel(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN做图像编码器，得到整体特征；LSTM做文本解码器，输出图像描述序列\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocabulary_size, in_channels, image_embed_dim, text_embed_dim, hidden_size=256,\n",
    "                 pretrained_embeddings=None):\n",
    "        super(ImageTextModel, self).__init__()\n",
    "\n",
    "        self.image_embed = ConvNet(in_channels, image_embed_dim)  # CNN整体表示\n",
    "\n",
    "        if pretrained_embeddings is not None:\n",
    "            assert pretrained_embeddings.shape == (vocabulary_size, text_embed_dim), \"预训练嵌入向量尺寸不匹配\"\n",
    "            self.text_embed = nn.Embedding.from_pretrained(torch.FloatTensor(pretrained_embeddings), freeze=False)\n",
    "        else:\n",
    "            self.text_embed = nn.Embedding(vocabulary_size, text_embed_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(image_embed_dim + text_embed_dim, hidden_size, num_layers=2, batch_first=True)\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_size, vocabulary_size)\n",
    "\n",
    "    def forward(self, images, seq, hidden_state: tuple = None, image_feats=None):\n",
    "        \"\"\"\n",
    "        将图像整体特征嵌入与拼接到序列的每一个时间步上，构成RNN的原始输入\n",
    "        :param images: (batch, channels, img_size, img_size)\n",
    "        :param seq: (batch, seq_len)\n",
    "        :param hidden_state: RNN（LSTM）隐含状态: (h, c)\n",
    "        :param image_feats: 图像特征\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if image_feats is None:\n",
    "            image_feats = self.image_embed(images)  # (batch, image_embed_dim)\n",
    "\n",
    "        image_embed = image_feats.unsqueeze(1).repeat(1, seq.size(1), 1)  # (batch, seq_len, image_embed_dim)\n",
    "\n",
    "        text_embed = self.text_embed(seq)  # (batch, seq_len, text_embed_dim)\n",
    "        embeddings = torch.cat((image_embed, text_embed), dim=2)  # -> (batch, seq_len, text_embed_dim+image_embed_dim)\n",
    "\n",
    "        lstm_output, hidden_state = self.lstm(embeddings, hidden_state)  # (batch, seq_len, hidden_size)\n",
    "\n",
    "        output = self.fc_out(lstm_output)  # (batch, seq_len, vocabulary_size)\n",
    "        return output, hidden_state, image_feats\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f22ef5a3155ee69"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 三、定义训练、预测和验证函数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2532d84d90bae54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, save_path, device, epochs=10,\n",
    "          save_interval=120, pretrained_weights=None, experiment_name='experiment'):\n",
    "    writer = SummaryWriter(f'runs/{experiment_name}')\n",
    "\n",
    "    os.makedirs(os.path.split(save_path)[0], exist_ok=True)\n",
    "\n",
    "    if pretrained_weights:\n",
    "        model.load_state_dict(torch.load(pretrained_weights))\n",
    "\n",
    "    # training loop\n",
    "    p_bar = tqdm(range(epochs))\n",
    "    model = model.to(device)\n",
    "    save_interval = timedelta(seconds=save_interval)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in p_bar:\n",
    "        running_loss = 0.0\n",
    "\n",
    "        last_save_time = datetime.now()\n",
    "        for batch_idx, (image, seq, seq_len) in enumerate(train_loader):\n",
    "            image = image.to(device)  # (batch, c, img_sz, img_sz)\n",
    "            seq = seq.to(device)  # (batch, seq_len + 1)\n",
    "\n",
    "            input_seq = seq[:, :-1]  # (batch, seq_len)\n",
    "            target_seq = seq[:, 1:]  # (batch, seq_len)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            prediction, _, _ = model(image, input_seq)  # (batch, seq_len - 1, vocabulary_size)\n",
    "            _, _, vocab_size = prediction.shape\n",
    "            loss = criterion(prediction.view(-1, vocab_size), target_seq.contiguous().view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # autosave\n",
    "            if datetime.now() - last_save_time > save_interval:\n",
    "                last_save_time = datetime.now()\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "\n",
    "            # 记录结果\n",
    "            running_loss += loss.item()\n",
    "            p_bar.set_postfix(progress=f'{(batch_idx + 1)} / {len(train_loader)}',\n",
    "                              loss=f'{running_loss / (batch_idx + 1):.4f}',\n",
    "                              last_save_time=last_save_time)\n",
    "            writer.add_scalar('Loss/train', running_loss / (batch_idx + 1), epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def predict(model, image, transform, vocab, device, max_length):\n",
    "    model.to(device)\n",
    "    image = transform(image).unsqueeze(0).to(device)  # -> (1, channel, img_size, img_size)\n",
    "    seq = [vocab.start]\n",
    "\n",
    "    h, feats = None, None\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            inp = torch.tensor([seq[-1]], dtype=torch.long, device=device).unsqueeze(0)\n",
    "            output, h, feats = model(image, inp, h, feats)\n",
    "\n",
    "        predicted = output.argmax(2)[:, -1]\n",
    "        seq.append(predicted.item())\n",
    "        if predicted.item() == vocab.end:\n",
    "            break\n",
    "\n",
    "    return vocab.decode(seq)\n",
    "\n",
    "def validate(model, val_set, transform, vocab, device, max_length):\n",
    "    pass\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "205d93b697b64a00"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 四、模型训练和结构超参数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bbfebeb39d830fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =========== Example parameters for the model ===========\n",
    "img_size = 256  # Size of the input image\n",
    "in_channels = 3  # Number of input channels (for RGB images)\n",
    "text_emb_size = 96  # Text Embedding size\n",
    "img_emb_size = 64  # Image Embedding size\n",
    "hidden_size = 256  # LSTM Hiddem size\n",
    "dropout = 0.1  # Dropout rate\n",
    "lr = 5e-4  # Learning rate\n",
    "epochs = 500  # train epochs\n",
    "batch_size = 64  # Batch size\n",
    "seq_length = 128  # Max length of the caption sequence\n",
    "save_path = 'models/model_lstm.pth'\n",
    "experiment_name = 'fashion_description_lstm'\n",
    "vocabulary_path = 'vocabulary/vocab.json'\n",
    "word2vec_cache_path = 'vocabulary/word2vec.npy'\n",
    "dataset_root = 'data/deepfashion-multimodal'\n",
    "train_labels_path = 'data/deepfashion-multimodal/train_captions.json'\n",
    "test_labels_path = 'data/deepfashion-multimodal/test_captions.json'\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5c9a86c99503019"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 五、构建词表，图像预处理变换"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8cefa98ae6d3d37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ==================== Build Vocabulary ====================\n",
    "\n",
    "vocabulary = Vocabulary(vocabulary_path)\n",
    "\n",
    "# ==================== Define image transforms ====================\n",
    "\n",
    "transform = Compose([\n",
    "    Resize((img_size, img_size)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c76a3bb6c5eb4e1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 六、初始化/加载模型、数据集"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "407de2503aa14a1e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ==================== Initialize the model ====================\n",
    "model = ImageTextModel(len(vocabulary), in_channels, img_emb_size, text_emb_size, hidden_size,\n",
    "                       pretrained_embeddings=vocabulary.get_word2vec(cache_path=word2vec_cache_path)).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "# ==================== Prepare for training ====================\n",
    "train_set = ImageTextDataset(dataset_root,\n",
    "                             train_labels_path,\n",
    "                             vocabulary=vocabulary,\n",
    "                             max_seq_len=seq_length,\n",
    "                             transform=transform,\n",
    "                             max_cache_memory=32 * 1024 ** 3)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bcf3e7a4af25c44"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 七、开始训练"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ca7c5e965b9af94"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# =================== Start Training ===================\n",
    "train(model, train_loader, criterion, optimizer,\n",
    "      save_path=save_path,\n",
    "      device=device,\n",
    "      epochs=epochs,\n",
    "      experiment_name=experiment_name)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e47bfc00e577421d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 八、模型推理和评估"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "286c5d704c7fd442"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#  =================== Model Inference ===================\n",
    "image_path, caption = train_set.sample()\n",
    "\n",
    "caption_generated = predict(model, Image.open(image_path), transform, vocabulary, device, seq_length)\n",
    "print(caption, '\\n\\n', caption_generated)\n",
    "print('bleu score: ', bleu(caption_generated, caption, vocabulary))\n",
    "print('rouge-l score: ', rouge_l(caption_generated, caption, vocabulary))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
